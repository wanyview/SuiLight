"""
SuiLight Knowledge Salon - LLM å·¥å‚
æ”¯æŒå¤šç§ LLM åç«¯ (å…è´¹/æœ¬åœ°/äº‘ç«¯)
"""

import os
from typing import List, Dict, Optional
from dataclasses import dataclass
from enum import Enum
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LLMProvider(Enum):
    """LLM æä¾›å•†"""
    MOCK = "mock"           # å…è´¹ Mock æ¨¡å¼
    OLLAMA = "ollama"       # æœ¬åœ° Ollama
    GROQ = "groq"          # Groq å…è´¹ tier
    OPENAI = "openai"       # OpenAI (GPT-4)
    MINIMAX = "minimax"     # MiniMax
    ANTHROPIC = "anthropic" # Claude


@dataclass
class LLMConfig:
    """LLM é…ç½®"""
    provider: str = "mock"  # é»˜è®¤ä½¿ç”¨å…è´¹ Mock
    api_key: str = ""
    base_url: str = ""
    model: str = ""
    temperature: float = 0.7
    max_tokens: int = 2000
    
    # Ollama æœ¬åœ°
    ollama_host: str = "http://localhost:11434"


class LLMClient:
    """
    LLM å®¢æˆ·ç«¯å·¥å‚
    
    æ”¯æŒå¤šç§åç«¯ï¼Œè‡ªåŠ¨åˆ‡æ¢
    """
    
    def __init__(self, config: LLMConfig = None):
        self.config = config or LLMConfig()
        self.provider = self.config.provider
        
        # åˆå§‹åŒ–å„åç«¯
        self.client = None
        self._init_client()
    
    def _init_client(self):
        """åˆå§‹åŒ–å®¢æˆ·ç«¯"""
        if self.provider == LLMProvider.MOCK.value:
            logger.info("ğŸ”§ ä½¿ç”¨ Mock æ¨¡å¼ (å…è´¹)")
            self.client = MockLLM()
            
        elif self.provider == LLMProvider.OLLAMA.value:
            logger.info(f"ğŸ”§ ä½¿ç”¨ Ollama (æœ¬åœ°: {self.config.ollama_host})")
            self.client = OllamaLLM(self.config)
            
        elif self.provider == LLMProvider.GROQ.value:
            logger.info("ğŸ”§ ä½¿ç”¨ Groq (å…è´¹ tier)")
            self.client = GroqLLM(self.config)
            
        elif self.provider == LLMProvider.OPENAI.value:
            logger.info("ğŸ”§ ä½¿ç”¨ OpenAI GPT")
            self.client = OpenAILLM(self.config)
            
        elif self.provider == LLMProvider.MINIMAX.value:
            logger.info("ğŸ”§ ä½¿ç”¨ MiniMax")
            self.client = MiniMaxLLM(self.config)
            
        else:
            logger.warning(f"æœªçŸ¥ provider: {self.provider}, ä½¿ç”¨ Mock")
            self.client = MockLLM()
    
    def chat(
        self,
        messages: List[Dict[str, str]],
        system_prompt: str = None,
        temperature: float = None,
        max_tokens: int = None
    ) -> str:
        """èŠå¤©æ¥å£"""
        return self.client.chat(
            messages=messages,
            system_prompt=system_prompt,
            temperature=temperature or self.config.temperature,
            max_tokens=max_tokens or self.config.max_tokens
        )
    
    def embedding(self, texts: List[str]) -> List[List[float]]:
        """åµŒå…¥å‘é‡"""
        return self.client.embedding(texts)


# ============ å„åç«¯å®ç° ============

class BaseLLM:
    """LLM åŸºç±»"""
    
    def chat(self, messages, system_prompt=None, temperature=0.7, max_tokens=2000) -> str:
        raise NotImplementedError
    
    def embedding(self, texts: List[str]) -> List[List[float]]:
        return [[0.0] * 384 for _ in texts]  # è¿”å›é›¶å‘é‡


class MockLLM(BaseLLM):
    """å…è´¹ Mock æ¨¡å¼ (æ— éœ€ API Key)"""
    
    def chat(self, messages, system_prompt=None, temperature=0.7, max_tokens=2000) -> str:
        # æå–ç”¨æˆ·æ¶ˆæ¯
        user_msg = ""
        for msg in reversed(messages):
            if msg.get("role") == "user":
                user_msg = msg.get("content", "")
                break
        
        if not user_msg:
            return "ä½ å¥½ï¼æˆ‘æ˜¯çŸ¥è¯†æ²™é¾™çš„ AI åŠ©æ‰‹ã€‚"
        
        # æ™ºèƒ½å›å¤
        return f"ã€çŸ¥è¯†æ²™é¾™ã€‘{user_msg}\n\nè¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå›å¤ã€‚\n\né…ç½®çœŸå®çš„ LLM åç«¯åå¯è·å¾—æ›´æ™ºèƒ½çš„å›ç­”ï¼š\n- Ollama (æœ¬åœ°å…è´¹)\n- Groq (å…è´¹ tier)\n- OpenAI (ä»˜è´¹)"
    
    def embedding(self, texts: List[str]) -> List[List[float]]:
        import numpy as np
        return [np.random.rand(384).tolist() for _ in texts]


class OllamaLLM(BaseLLM):
    """Ollama æœ¬åœ°æ¨¡å‹ (å…è´¹)"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.base_url = config.ollama_host
        self.model = config.model or "llama3"
        
        try:
            import requests
            self.requests = requests
        except ImportError:
            logger.warning("requests åº“æœªå®‰è£…")
            self.requests = None
    
    def chat(self, messages, system_prompt=None, temperature=0.7, max_tokens=2000) -> str:
        if not self.requests:
            return MockLLM().chat(messages)
        
        try:
            # æ„å»ºæ¶ˆæ¯
            all_messages = []
            if system_prompt:
                all_messages.append({"role": "system", "content": system_prompt})
            all_messages.extend([{"role": m["role"], "content": m["content"]} for m in messages])
            
            response = self.requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "messages": all_messages,
                    "stream": False,
                    "options": {
                        "temperature": temperature,
                        "num_predict": max_tokens
                    }
                },
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                return data.get("response", "")
            else:
                logger.error(f"Ollama é”™è¯¯: {response.status_code}")
                return MockLLM().chat(messages)
                
        except Exception as e:
            logger.error(f"Ollama è°ƒç”¨å¤±è´¥: {e}")
            return MockLLM().chat(messages)
    
    def embedding(self, texts: List[str]) -> List[List[float]]:
        """Ollama embedding"""
        import numpy as np
        return [np.random.rand(384).tolist() for _ in texts]


class GroqLLM(BaseLLM):
    """Groq å…è´¹ tier (å…è´¹é«˜é€Ÿ)"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.api_key = config.api_key or os.getenv("GROQ_API_KEY", "")
        
        try:
            from groq import Groq
            self.client = Groq(api_key=self.api_key)
        except ImportError:
            logger.warning("groq åº“æœªå®‰è£…: pip install groq")
            self.client = None
    
    def chat(self, messages, system_prompt=None, temperature=0.7, max_tokens=2000) -> str:
        if not self.client:
            return MockLLM().chat(messages)
        
        try:
            # æ„å»ºæ¶ˆæ¯
            all_messages = []
            if system_prompt:
                all_messages.append({"role": "system", "content": system_prompt})
            all_messages.extend([{"role": m["role"], "content": m["content"]} for m in messages])
            
            response = self.client.chat.completions.create(
                model="llama3-8b-8192",  # Groq å…è´¹æ¨¡å‹
                messages=all_messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"Groq è°ƒç”¨å¤±è´¥: {e}")
            return MockLLM().chat(messages)
    
    def embedding(self, texts: List[str]) -> List[List[float]]:
        import numpy as np
        return [np.random.rand(384).tolist() for _ in texts]


class OpenAILLM(BaseLLM):
    """OpenAI GPT (ä»˜è´¹)"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.api_key = config.api_key or os.getenv("OPENAI_API_KEY", "")
        
        try:
            from openai import OpenAI
            self.client = OpenAI(api_key=self.api_key)
        except ImportError:
            logger.warning("openai åº“æœªå®‰è£…")
            self.client = None
    
    def chat(self, messages, system_prompt=None, temperature=0.7, max_tokens=2000) -> str:
        if not self.client:
            return MockLLM().chat(messages)
        
        try:
            all_messages = []
            if system_prompt:
                all_messages.append({"role": "system", "content": system_prompt})
            all_messages.extend([{"role": m["role"], "content": m["content"]} for m in messages])
            
            response = self.client.chat.completions.create(
                model=self.config.model or "gpt-4",
                messages=all_messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"OpenAI è°ƒç”¨å¤±è´¥: {e}")
            return MockLLM().chat(messages)
    
    def embedding(self, texts: List[str]) -> List[List[float]]:
        if not self.client:
            return super().embedding(texts)
        
        try:
            response = self.client.embeddings.create(
                model="text-embedding-3-small",
                input=texts
            )
            return [d.embedding for d in response.data]
        except Exception as e:
            logger.error(f"OpenAI embedding å¤±è´¥: {e}")
            return super().embedding(texts)


class MiniMaxLLM(BaseLLM):
    """MiniMax (å›½å†…)"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.api_key = config.api_key or os.getenv("MINIMAX_API_KEY", "")
        self.base_url = config.base_url or "https://api.minimax.io"
        
        try:
            from openai import OpenAI
            self.client = OpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )
        except ImportError:
            logger.warning("openai åº“æœªå®‰è£…")
            self.client = None
    
    def chat(self, messages, system_prompt=None, temperature=0.7, max_tokens=2000) -> str:
        if not self.client:
            return MockLLM().chat(messages)
        
        try:
            all_messages = []
            if system_prompt:
                all_messages.append({"role": "system", "content": system_prompt})
            all_messages.extend([{"role": m["role"], "content": m["content"]} for m in messages])
            
            response = self.client.chat.completions.create(
                model=self.config.model or "MiniMax-M2.1",
                messages=all_messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"MiniMax è°ƒç”¨å¤±è´¥: {e}")
            return MockLLM().chat(messages)
    
    def embedding(self, texts: List[str]) -> List[List[float]]:
        import numpy as np
        return [np.random.rand(384).tolist() for _ in texts]


# ============ ä¾¿æ·å‡½æ•° ============

def create_llm_client(
    provider: str = "mock",
    api_key: str = None,
    model: str = None
) -> LLMClient:
    """
    åˆ›å»º LLM å®¢æˆ·ç«¯
    
    Args:
        provider: æä¾›å•† (mock/ollama/groq/openai/minimax)
        api_key: API Key
        model: æ¨¡å‹åç§°
        
    Returns:
        LLM å®¢æˆ·ç«¯
    """
    config = LLMConfig(
        provider=provider,
        api_key=api_key or os.getenv(f"{provider.upper()}_API_KEY", ""),
        model=model
    )
    return LLMClient(config)


def get_free_llm_options() -> Dict:
    """
    è·å–å…è´¹ LLM é€‰é¡¹
    """
    return {
        "mock": {
            "name": "Mock æ¨¡å¼",
            "description": "å…è´¹ï¼Œæ— éœ€ API Key",
            "cost": "$0",
            "setup": "æ— éœ€è®¾ç½®",
            "quality": "â­â­"
        },
        "ollama": {
            "name": "Ollama æœ¬åœ°",
            "description": "å®Œå…¨å…è´¹ï¼Œæœ¬åœ°è¿è¡Œ",
            "cost": "$0 (éœ€æœ¬åœ° GPU/CPU)",
            "setup": "å®‰è£… Ollama + ä¸‹è½½æ¨¡å‹",
            "quality": "â­â­â­â­"
        },
        "groq": {
            "name": "Groq å…è´¹ tier",
            "description": "å…è´¹é«˜é€Ÿï¼ŒLlama 3",
            "cost": "å…è´¹ tier",
            "setup": "æ³¨å†Œ groq.cloud è·å– API Key",
            "quality": "â­â­â­â­â­"
        }
    }


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("=" * 50)
    print("SuiLight Knowledge Salon - LLM é€‰é¡¹")
    print("=" * 50)
    
    options = get_free_llm_options()
    for key, info in options.items():
        print(f"\n{key.upper()}: {info['name']}")
        print(f"  è´¹ç”¨: {info['cost']}")
        print(f"  è´¨é‡: {info['quality']}")
        print(f"  è®¾ç½®: {info['setup']}")
    
    print("\n" + "=" * 50)
    print("ä½¿ç”¨å…è´¹ Mock æ¨¡å¼å¯åŠ¨...")
    client = create_llm_client("mock")
    print(client.chat([{"role": "user", "content": "ä½ å¥½"}))
